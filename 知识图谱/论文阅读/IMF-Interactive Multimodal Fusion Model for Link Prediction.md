# IMF: Interactive Multimodal Fusion Model for Link Prediction

> 链接预测的交互式多模态融合模型

---

[toc]

---

链接预测旨在识别知识图中潜在的缺失三元组。为了得到更好的结果，最近的一些研究引入了多模态信息来进行链接预测。然而，这些方法单独利用多模态信息，忽略了不同模态之间复杂的相互作用。在本文中，我们旨在更好地建模跨模态信息，从而引入一种新的交互式多模态融合(IMF)模型来整合来自不同模态的知识。为此，我们提出了一个两阶段的多模态融合框架，以保留模态特定知识，并利用不同模态之间的互补性。我们的多模态融合模块不是直接将不同的模态投射到一个统一的空间中，而是限制了不同模态的独立表示，同时利用双线性池进行融合，并将对比学习作为额外的约束。此外，决策融合模块提供所有模式预测的学习加权平均，以更好地融合不同模式的互补性。通过对几个真实世界数据集的实证评估，我们的方法已被证明是有效的。实现代码可在https://github.com/HestiaSky/IMF-Pytorch上在线获得。

## 1 介绍

本文提出了一种新的交互式多模态融合模型(IMF)，用于知识图上的多模态链路预测。IMF可以单独学习每个模态的知识，并通过两阶段融合的方式对不同模态之间复杂的相互作用进行联合建模，类似于上文介绍的人类自然识别过程。在**多模态融合阶段，我们采用双线性融合机制，通过对比学习充分捕捉多模态特征之间复杂的相互作用**。对于基本链接预测模型，我们利用关系信息作为上下文对每个模态中的三元组作为预测进行排序。在最后的决策融合阶段，我们将不同模式的预测进行整合，利用互补信息进行最终的预测。本文的贡献总结如下:

- 我们提出了一种新的两阶段融合模型IMF，它可以有效地整合不同模式的互补信息进行链接预测；
- 我们设计了一个有效的多模态融合模块，以捕获双线性交互与对比学习，共同建模的共性和互补性；
- 我们通过对四个广泛使用的数据集进行多模态链接预测的大量实验，证明了IMF的有效性和泛化性；



## 2 方法过程

<img src="./IMF-Interactive%20Multimodal%20Fusion%20Model%20for%20Link%20Prediction.assets/image-20240109214159026.png" alt="image-20240109214159026" style="zoom:50%;" />

## 2.1 结构层次

为了充分利用不同模态之间复杂的相互作用，我们提出了一种两阶段融合模型，而不是简单地在统一的向量空间中单独考虑多模态信息。如图2所示，IMF由四个关键组件组成:

1. 模态特定编码器用于提取结构、视觉和文本特征，作为多模态融合阶段的输入；
2. 多模态融合模块(Multimodal Fusion Module)是融合的第一个阶段，它基于Tucker分解和对比学习有效地模拟了不同模态之间的双线性相互作用；
3. 上下文关系模型计算上下文实体表示的相似性，以形成三分作为决策融合阶段特定于模式的预测；
4. 决策融合模块是第二个融合阶段，考虑了结构、视觉、文本和多模态模型的所有相似分数，以做出最终预测；

## 2.2 模态编码

在本小节中，我们首先介绍用于不同模式的预训练编码器。这些编码器在训练过程中没有进行微调，我们将它们视为固定的特征提取器，以获得特定于模态的实体表示。请注意，IMF是一个通用框架，用其他最新编码器替换它们或为IMF添加新模式编码器是很简单的。

### 2.2.1 结构编码

从最基本的角度来看，即KG的结构信息，我们采用了具有**TransE损失的图注意网络(GAT)**。

### 2.2.2 视觉编码

视觉特征具有很强的表现力，同时提供了与传统KGs不同的知识视角。为了有效提取视觉特征，我们利用在**ImageNet3上预训练的VGG162**，得到如下对应实体的图像嵌入。具体来说，我们将softmax操作前最后一个隐藏层的输出作为视觉特征，即4096维向量。

### 2.2.3 文本编码

实体描述所包含的知识要比纯粹的KGs丰富得多，但也更加复杂，为了充分提取复杂的知识，我们采用**BERT**作为文本编码器，它具有很强的表达能力，可以得到相应实体的描述嵌入。文本特征为768维向量，即预训练BERT-Base模型的汇总输出。

## 2.3 模态融合

多模态融合阶段旨在有效地获得多模态表征，以充分捕捉不同模态之间复杂的相互作用。许多现有的多模态融合方法在VQA (Visual Question answer)等任务中都取得了很好的效果。然而，它们大多是通过情态投射或跨情态注意来寻找共性以获得更精确的表征。这些类型的方法在不同的模态中会丢失独特的信息，并且不能在模态之间实现充分的交互。

为此，我们提出采用双线性模型作为多模态信息融合的基石，该模型具有较强的实现全参数交互的能力。具体来说，我们扩展了Tucker分解，它将张量分解为一个核心张量，每个张量由矩阵和每个模态变换为4模态因子，如式所示
$$
p=(((p_c\times M_s)\times M_v)\times M_t) \times M_d
$$
在这种情况下，实体嵌入首先被投射到一个低维空间，然后与核心张量$p_c$融合。接下来，我们通过分解核心张量$p_c$来进一步降低计算复杂度，将所有模态的表示合并到一个具有元素积的统一空间中。

然而，多模态双线性融合没有边界限制，最终预测结果产生的梯度只能隐式指导参数学习。为了缓解这个问题，我们添加了约束来限制同一实体的不同模态表示之间的相关性，使其更强。

因此，我们进一步利用不同实体和模式之间的对比学习作为正则化的额外学习目标。在对比学习设置中，我们将不同模态的同一实体的成对表征作为正样本，将不同实体的成对表征作为负样本。如图所示，我们的目标是限制负样本的距离大于正样本的距离，以增强多模态融合，即:
$$
d(f(x),f(x^+))<<d(f(x),f(x^-))
$$

## 2.4 上下文关系模型

在获得每个模态和多模态的表示后，我们设计了一个上下文关系模型，该模型将三元组中的关系作为评分的上下文信息，以获得预测。注意，这个关系模型可以很容易地被任何评分函数(如TransE)取代。

由于KGs中关系的多样性和复杂性，我们认为提高参数交互程度对于更好地建模关系三元组至关重要。参数相互作用程度是指每个参数与所有其他参数的计算比率。例如，点积可以达到1/p度，叉积可以达到(p−1)/p度。基于这一假设，我们建议使用实体和关系嵌入之间的双线性外积来将上下文信息整合到实体表示中。我们的上下文关系模型不像以前的研究那样将关系作为输入，而是利用关系在实体嵌入的转换矩阵中提供上下文。然后，使用上下文变换矩阵对实体嵌入进行投影，得到上下文嵌入，用于计算与所有候选实体的相似度。学习目标是最小化二元交叉熵损失。

## 2.5 决策融合

现有的多模态方法主要是将不同的模态表示投射到一个统一的空间中，并利用模态之间的共性进行预测，无法保留模态特有的知识。我们通过联合学习和结合不同模式的预测来缓解决策融合阶段的这一问题，以进一步利用互补性。

在多模态设置下，我们为每个模态分配不同的上下文关系模型，并利用它们自己的结果在不同的视图下进行训练。

为了更好地说明IMF的整个训练过程，我们用优化算法的伪代码来描述。如算法1所示，我们首先获得预训练的结构、视觉和文本编码器，并将其用于实体嵌入(第3-5行)。由于预训练模型比IMF大得多，也复杂得多，因此没有进行微调，其输出直接用作IMF的输入。通过多模态融合获得多模态嵌入，同时应用对比学习进一步增强融合阶段(第9-11行)。在训练过程中，每个模态通过特定于模态的评分器提供自己的预测和损失(第12行)，然后基于包括多模态在内的所有模态计算联合预测和损失(第14行)。

